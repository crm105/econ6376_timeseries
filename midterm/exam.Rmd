---
title: "Time Series Mid-Term Exam"
author: "Chris Montgomery"
date: "11/28/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(ggplot2)
library(ggfortify)
library(forecast)
```

<font face = "bold", size =16> Problem II </font><p>
<font size = 8> Consider the following equation: </font>
$$x_t = 3 - .5x_{t-1}  $$


a.) The above function is stationary in the mean. We know this because we have an AR(1) series with an $a_1$ parameter < 1. The long run mean can be obtained from the function: 
$$Mean(x_t) =  a_0/(1-a1)$$
$$= 3/(1+ .5) $$
$$ = 2$$

Simulation further demonstrates the mean of the above series converges to 2

```{r} 
set.seed(3)
x <- rnorm(10)
x[1] = 0
for (t in 2:10){
  x[t] <- 3 -.5*x[t-1]
  
}
autoplot(ts(x))

x[4]
x
```




While the long run mean of the series is 2, we should note that the series requires several iterations before reaching equillibrium. For example, the above simulation shows $x_4$ = 2.25. In fact, the series continuously oscillates around the long run mean as it converges to the mean, a characteristic of the negative $a_1$ parameter. 






<font face = "bold", size = 16>Problem III </font> 

$$x_t = 3 + w_t = .5w_{t-1}$$
The above function represents a first-order moving average process,
where $W$ represents a white noise sequence (0, $\sigma^2$). We know this function is mean stationary because it is composed of a constant added to a white noise process with an unconditional mean of 0. Remember, for all values of t $$ E[w_t] = E[ w_t + .5w_{t-1}] \\= E[w_t] + E[w_{t-1}]\\ = 0$$ and $$E[x_t] = 3 + E[w_t]\\ = 3$$ Thus, given the above MA(1) process with an intercept term of 3, the expected unconditional mean is 3. Simulating this process supports the expectation of stationarity and the theorized mean of 3. <p>

```{r}
set.seed(4)
x <- w <- rnorm(100)
for (t in 2:100){
  x[t] <- 3 + w[t] + (.5 * w[t-1])
}
plot(x, type = "l")
mean(x)
```
Augmented Dickey Fuller test further demonstrates that the MA(1) process is stationary. We reject the null hypothesis of non-stationarity at the .1 level. Simulating with additional observations reduces the p-value further, which is to be expected. 

```{r}
adf.test(x) 
```

A theoretic confidence interval can be derived from the function's expected mean and variance. As demonstrated above, we know the expected mean is 3. The expected variance of the function can be defined by $VAR[x_t] = \sigma^2(\beta_0^2 + \beta_1^2)$ where $\sigma^2$ represents the variance of the white noise process and $\beta_0$ and $\beta_1$ represent the coefficients of each moving average term. With this we can derive the expected standard deviation of the function and by extension, a theoretical confidence interval.

$$
\begin{aligned} 
VAR[x_t] = \sigma^2 (\beta_0^2 + \beta_1^2) \\
   = 1( 1^2 + .5^2)  \\
  = 1.25 \\ 
  \end{aligned}
  $$
  $$
  \begin{aligned}
  Standard Deviation [x_t] = \sqrt{1.25} \\ 
  \end{aligned}
  $$
  $$
  \begin{aligned}
  95\% Confidence Interval = 3\pm 1.96 *\sqrt{1.25} \\
  = (.8,5.19)
\end{aligned}
$$
Calculating the theoretical 95% confidence interval of the function near approximates the CI observed from the simulated data. 
```{r}
#Calculate the 95% CI of the simulated function

upperbound <- mean(x) + (1.96 *var(x))
lowerbound <- mean(x) - (1.96 *var(x))

print(c(lowerbound,upperbound))
```

<font face = "bold" size = 16> Part IV </font>


The main focus in this problem is to provide a model that can best fit a series that has conditional heteroskedacity. The following will demonstrate techniques for identifying autocorrelation of variance and fitting a General Autoregressive Conditional Heteroskedacity (GARCH) model to ARIMA residuals.  

<font face = "bold"> step 1.) Load and inspect the data </font> <p>

```{R}
df <- read.csv("ERCOT LZ North January 2014.csv")
ts <- ts(df$Settlement.Point.Price)

autoplot(ts) + theme_bw()+
labs(title="ERCOT LZ North Electricity Prices", subtitle="($/MWh)", y="", x="", caption="")+
  theme(plot.title = element_text( face = "bold", size = "19.5"))+
  theme(plot.subtitle = element_text( face = "italic", size = "12"))+
  theme(axis.text.x =element_text(size=13.5, vjust = -.75)
        , axis.text.y = element_text(size = 13.5 ),
        legend.title=element_text(face = "bold",size=11), 
        legend.text=element_text(size=10, face = "bold"),
        plot.caption = element_text (size = 12))

```
The above series represents per megawatt hour electricity prices update every 15 seconds. In total, the series includes 31 days or 2976 observations. Augmented Dickey Fuller and Phillips-Perron tests suggest that the series is stationary in the mean. ACF and PACF plots suggest the absence of unit roots. 

```{r}
adf.test(ts); PP.test(ts)

acf(ts)
pacf(ts)


```
<font face = "bold"> Step 2: Fit an ARIMA to the series </font> <p>

The auto.arima function featured in the R forecast package determined the best fitting model to be an ARMA (1,2). ACF and PCF plots of the residuals suggest that some autocorrelation in the residuals remain. Furthermore, the ACF of the square of the residuals shows high autocorrelation even after the model fit. This suggests the residuals series should be fit with a GARCH model.  
```{r}

arima.fit <- auto.arima(ts, allowdrift = TRUE, allowmean = TRUE,
                        trace = TRUE)
summary(arima.fit)

acf(arima.fit$residuals) ; pacf(arima.fit$residuals)

acf(arima.fit$residuals^2); pacf(arima.fit$residuals^2)

```
The below code fits a GARCH model to the residuals. Put simply, the GARCH model will effectively fit an ARMA process to the residuals of the preceding ARIMA model. ACF plots of the GARCH residuals show that the GARCH(0,1) model fits better than GARCH(1,1). However, both models yielded statistically significant coefficients.

```{r}

a.garch1 <- garch(arima.fit$residuals, order=c(0,1), grad="numerical",
                  trace=FALSE)
summary(a.garch1)
acf(a.garch1$residuals[-1])

a.garch2 <- garch(arima.fit$residuals, order=c(1,1), grad="numerical",
                  trace=FALSE)
summary(a.garch2)
acf(a.garch2$residuals[-1])

?garch
```

Given the presence of significant coefficients with the GARCH model, 