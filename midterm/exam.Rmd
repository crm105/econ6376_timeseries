---
title: "Time Series Mid-Term Exam"
author: "Chris Montgomery"
date: "11/28/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(ggplot2)
library(ggfortify)
library(forecast)
library(dplyr)

```
<font face = "bold", size = 12> Problem I </font> <p>

The below equation models a random walk process.

$$y_t = y_{t-1} + w_t $$ 
Assuming a mean of zero, variance $\sigma^2$, and zero autocorrelation for the white noise term, the uncondutional mean of the series can be shown as follows: 


$$E[y_t] = E[y_{t-1}] + E[w_t] $$
$$E[y_{t-1}] = E[y_{t-2}] + E[w_{t-1}] $$  
$$E[y_{t}] = E[y_{t-2}] + E[w_t] + E[w_{t-1}] $$
$$E[y_{t}] = E[w_t] + E[w_{t-1}] + E[w_{t-2}].... + E[w_0]$$
$$ E[y_t] = 0 $$ 

The value of a random walk at any time t is simply the summation of the expectation of the random noise term at time t and all previous noise terms. Based on the above assumption of a non-autocorrelated mean zero white noise series, we know the expectation of each noise term is zero. Thus the unconditional mean of a random walk at time t, is likewise zero. The unconditional variance of the random walk process is simply t$\sigma^2$. This represents the summation of all the variances of the white noise terms in the sequence. 

```{r}
set.seed (3)

y <-w  <- rnorm(100)

for (t in 2:100){
  y[t] <- y[t-1] + w[t]
}

plot(y, type = "l")  

var (y[1:5])


```

The conditional mean and variance of the simulated series is -1.46 and .3 respectively at t = 5. <p>

If I were to believe an asset's price followed a random walk I would not attempt to forecast its future value. In a random walk model the only thing that affects the present value of a series is an unknown present shock. As such, it's impossible to use previous information to predict future values. This concept essentially underpins the Efficient Market Hypothesis.  


<font face = "bold", size =12> Problem II </font><p>
 Consider the following equation: 
$$x_t = 3 - .5x_{t-1}  $$


a.) The above function is stationary in the mean. We know this because we have an AR(1) series with an $a_1$ parameter < 1. The long run mean can be obtained from the function: 
$$Mean(x_t) =  a_0/(1-a1)$$
$$= 3/(1+ .5) $$
$$ = 2$$

Simulation further demonstrates the mean of the above series converges to 2

```{r} 
set.seed(3)
x <- rnorm(10)
x[1] = 0
for (t in 2:10){
  x[t] <- 3 -.5*x[t-1]
  
}
autoplot(ts(x))

x[4]
x
```




While the long run mean of the series is 2, we should note that the series requires several iterations before reaching equillibrium. For example, the above simulation shows $x_4$ = 2.25. In fact, the series continuously oscillates around the long run mean as it converges to the mean, a characteristic of the negative $a_1$ parameter. 

<font face = "bold", size = 14>Problem III </font> 

$$x_t = 3 + w_t = .5w_{t-1}$$
The above function represents a first-order moving average process,
where $W$ represents a white noise sequence (0, $\sigma^2$). We know this function is mean stationary because it is composed of a constant added to a white noise process with an unconditional mean of 0. Remember, for all values of t $$ E[w_t] = E[ w_t + .5w_{t-1}] \\= E[w_t] + E[w_{t-1}]\\ = 0$$ and $$E[x_t] = 3 + E[w_t]\\ = 3$$ Thus, given the above MA(1) process with an intercept term of 3, the expected unconditional mean is 3. Simulating this process supports the expectation of stationarity and the theorized mean of 3. <p>

```{r}
set.seed(4)
x <- w <- rnorm(100)
for (t in 2:100){
  x[t] <- 3 + w[t] + (.5 * w[t-1])
}
plot(x, type = "l")
mean(x)
```
Augmented Dickey Fuller test further demonstrates that the MA(1) process is stationary. We reject the null hypothesis of non-stationarity at the .1 level. Simulating with additional observations reduces the p-value further, which is to be expected. 

```{r}
adf.test(x) 
```

A theoretic confidence interval can be derived from the function's expected mean and variance. As demonstrated above, we know the expected mean is 3. The expected variance of the function can be defined by $VAR[x_t] = \sigma^2(\beta_0^2 + \beta_1^2)$ where $\sigma^2$ represents the variance of the white noise process and $\beta_0$ and $\beta_1$ represent the coefficients of each moving average term. With this we can derive the expected standard deviation of the function and by extension, a theoretical confidence interval.

$$
\begin{aligned} 
VAR[x_t] = \sigma^2 (\beta_0^2 + \beta_1^2) \\
   = 1( 1^2 + .5^2)  \\
  = 1.25 \\ 
  \end{aligned}
  $$
  $$
  \begin{aligned}
  Standard Deviation [x_t] = \sqrt{1.25} \\ 
  \end{aligned}
  $$
  $$
  \begin{aligned}
  95\% Confidence Interval = 3\pm 1.96 *\sqrt{1.25} \\
  = (.8,5.19)
\end{aligned}
$$
Calculating the theoretical 95% confidence interval of the function near approximates the CI observed from the simulated data. 
```{r}
#Calculate the 95% CI of the simulated function

upperbound <- mean(x) + (1.96 *var(x))
lowerbound <- mean(x) - (1.96 *var(x))

print(c(lowerbound,upperbound))
```

<font face = "bold" size = 14> Part IV </font>


The main focus in this problem is to provide a model that can best fit a series that has conditional heteroskedacity. The following will demonstrate techniques for identifying autocorrelation of variance and fitting a General Autoregressive Conditional Heteroskedacity (GARCH) model to ARIMA residuals.  

<font face = "bold"> step 1.) Load and inspect the data </font> <p>

```{R}
df <- read.csv("ERCOT LZ North January 2014.csv")
ts <- ts(df$Settlement.Point.Price)

autoplot(ts) + theme_bw()+
labs(title="ERCOT LZ North Electricity Prices", subtitle="($/MWh)", y="", x="", caption="")+
  theme(plot.title = element_text( face = "bold", size = "19.5"))+
  theme(plot.subtitle = element_text( face = "italic", size = "12"))+
  theme(axis.text.x =element_text(size=13.5, vjust = -.75)
        , axis.text.y = element_text(size = 13.5 ),
        legend.title=element_text(face = "bold",size=11), 
        legend.text=element_text(size=10, face = "bold"),
        plot.caption = element_text (size = 12))

```
<p> The above series represents per megawatt hour electricity prices update every 15 seconds. In total, the series includes 31 days or 2976 observations. Augmented Dickey Fuller and Phillips-Perron tests suggest that the series is stationary in the mean. ACF and PACF plots suggest the absence of unit roots. 

```{r}
adf.test(ts); PP.test(ts)
acf(ts)



```
<p> <font face = "bold"> Step 2: Fit an ARIMA to the series to isolate and inspect residuals</font> <p>

The auto.arima function featured in the R forecast package determined the best fitting model to be an ARMA (1,2). ACF and PCF plots of the residuals suggest that some autocorrelation in the residuals remain. Furthermore, the ACF of the square of the residuals shows high autocorrelation even after the model fit. This suggests the residuals series should be fit with a GARCH model.  <p>
```{r}

arima.fit <- auto.arima(ts, allowdrift = TRUE, allowmean = TRUE,
                        trace = TRUE)
summary(arima.fit)

acf(arima.fit$residuals) ; acf(arima.fit$residuals^2)

```
<p> <font face = "bold"> Step 3: Fit a GARCH Model to Residual Series </font> <p>The below code fits a GARCH model to the residuals. Put simply, the GARCH model will effectively fit an ARMA process to the residuals of the preceding ARIMA model. ACF plots of the GARCH residuals show that the GARCH(0,1) model fits better than GARCH(0,1). However, both models yielded statistically significant coefficients. With this model, the a1 coefficient can be interpreted as the autoregressive residual term, with b1 representing a residual moving average term. Tests of both a GARCH (0,1) and GARCH(1,1) model elucidate a significant a1 coefficient and insignificant b1. As such, we can conclue the electricity series to be conditionally heteroskedastic, following a GARCH (1,0) process. 

```{r}

a.garch1 <- garch(arima.fit$residuals, order=c(0,1), grad="numerical",
                  trace=FALSE)
summary(a.garch1)
acf(a.garch1$residuals, na.action = na.pass)

#Residuals show the 0,1 process is a better fit 
# a.garch2 <- garch(arima.fit$residuals, grad="numerical",
#                   trace=FALSE)
# summary(a.garch2)
# acf(a.garch2$residuals[-1], na.action = na.pass)
```

The output from the above GARCH model can be used to improve forecasts of the original electricity series. GARCH output can help estimate the conditional variance of the series over time. This in turn, can be used to update forecast prediction error of the original series forecast. As a result, the conditional variance of the forecast will scale with the original time series process.



<font size = 12, face = "bold"> PART V </font>
<p>

```{r}
# PART V #

wti <- read.csv("wti_prices.csv")
names(wti)[2] <- "wti"
wti$DATE <- as.Date(wti$DATE, format = "%Y-%m-%d")

brent <- read.csv("brent_prices.csv")
brent$DATE <- as.Date(brent$DATE, format = "%Y-%m-%d")



names(brent)[2] <- "brent"

df <- merge(brent, wti)
df <- na.omit(df)
df$spread <- df$wti - df$brent

ggplot(df, aes(x = DATE, y = wti))+ theme_bw()+
geom_line(color = "red")+ geom_line(aes(y = brent), color = "blue")+ 
  labs(title = "Brent and WTI Crude Price Series", subtitle = "Monthly 1987-2017", caption = "Source: St. Louis FRED")

```

Below contains code that attempts to elucidate whether Brent and WTI crude prices are integrated to the same order. To begin, ACF and PACF plots for both series share near identical structures, following decay patterns similar to the random walk model. This suggests both series may share the same I(1) order of integration. Augmented Dickey Fuller and Phillips-Perron tests show non-stationarity, confirming the I(1) order of integration hypothesis. Plotting the difference of the two series shows consistent white noise until approximately 250 observations. This could affect a co-integration test. 

```{r}

#Let's test each series order of integration 
#1987-2018 Range
  #Definitely appear to have a similar acf decay structure
  #Both suggestive of random walk. Near identical PACF as well
layout(1:2)
acf(df$wti); acf(df$brent)
pacf(df$wti); pacf(df$brent)

#ADF and PP tests further suggest we are I(1) for both series
adf.test(df$wti); adf.test(df$brent)
PP.test(df$wti); PP.test(df$brent)

 #First differences return both series to stationary
plot(diff(df$wti));
plot(diff(df$brent))

plot(df$wti - df$brent, type = "l")


```
The Phillips-Ouliaris test for the entire series lengths demonstrate that the full series are co-integrated. This should make sense intuitively as the principles of arbitrage suggest disturbances in one series should spillover to the other price series. 
```{r}

po.test(cbind(df$brent, df$wti))
```



```{r}
#Subset dataframe to 2005-2017 period



#Create the intervention term.
df$decline <- 0
df[df$DATE > "2010-11-01" ,]$decline = 1

df$reversal <- 0
df[df$DATE > "2012-05-19",]$reversal = 1

df$time <- df$id <- seq(nrow(df))
df$time <- df$time - df[df$DATE == "2012-05-01",]$time
df$post.int <- df$time * df$reversal

post <- df[df$DATE > "2018-01-01",]
df1 <- df[df$DATE > "2004-12-31" & df$DATE < "2018-01-01",]


#Baseline Regression 

lm1 <- lm(df1$wti~df1$brent)
#plot(df1$wti, type = "l"); lines(lm$fitted.values, type = "l", col="red")

# Baseline with time trend
lm2 <- lm(df1$wti ~ df1$brent + df1$id)

#Baseline with time trend and reversal dummy 
lm3 <- lm(df1$wti ~ df1$brent + df1$id + df1$reversal)

#Baseline with time trend reversal dummy and interaction between time 
#and reversal
lm4 <- lm(df1$wti ~ df1$brent + df1$id + df1$reversal + df1$post.int)

#lm3 + decline intervention term 
lm5 <-lm(df1$wti ~ df1$brent + df1$time + df1$reversal +
           df1$decline+df1$post.int)

#Run some regressions with our initial intervention date 

lm6 <- lm(df1$wti~df1$brent +df1$decline+df1$reversal +
            df1$post.int )


``` 

The above segment of code applies 6 different linear model specifications with the WTI price series representing the dependent variable. Each regression specification is shown below <p>

1.) $y_t = \beta_1brent_t$ 
<p>
2.) $y_t = \beta_1brent_t + \beta_2time$
<p>
3.) $y_t = \beta_1 brent_t + \beta_2 time + \beta_3reversal$ 
<p>
4.) $y_t = \beta_1 *brent_t + \beta_2time + \beta3reversal + \beta4post.intervention$ 
<p>
5.) $y_t = \beta_1 *brent_t + \beta_2time + \beta3reversal + \beta4post.intervention + \beta5decline$
<p>
6.) $y_t = \beta_1 *brent_t + \beta3reversal + \beta4post.intervention + \beta5decline$ 

<p>

Where $brent_t$ represents brent prices. $reversal$ and $post.intervention$ are model a post intervention dummy and its interaction with time. Specifications 5 and 6 include a $decline$ term, which represents a dummy intervention beginning in December 2010. The timing of the decline intervention term coincides with the increase in the spread between Brent and WTI crude caused by  the rise of the US shale boom. 

```{r}
# summary(lm1)
# summary(lm2)
# summary(lm3)
# summary (lm4)
# summary (lm5)
summary(lm6)

```

Output from the above regressions show that specification 6, which includes brent prices, a pipeline reversal intervention dummy, a reversal time interaction term, and a shale boom intervention dummy provided the best fit. The Adjusted $R^2$ value was .974, with an F-statistic of 1434. The coefficient on the brent term was .966, which can be interpreted as the price of WTI crude is estimated to be 96.6% the price of brent. This value is significantly closer to what arbitrage theory would suggest than the .83 coefficient found in the baseline specification. The coefficient of the decline term was -15.1, suggesting that following the shale boom WTI prices declined on average by \$15.10. The pipeline reversal dummy was found to be positive but statistically insignificant. This suggests that the initial effect of the pipeline reversal was rather small. However,  the intervention interaction with time was found to be positive and significant. Thus, WTI prices increased by about 20 cents per month following the pipeline reversal, following an initial increase of about \$1.30.  

```{r}

plot(lm6$residuals, type = "l")

plot(df1$wti, type = "l"); lines(x = seq(length(df1$wti)), y = lm6$fitted.values, type = "l", col = "red")

```

ACF plots of the preffered linear model demonstrate residual autocorrelation. This suggests that a model with ARMA errors could provide a better fit. In fact acf and fitted plots of an ARIMAX model featuring the  exogenous regressors from the preffered regression specification demonstrate a better fit with no significant residual autocorrelation. Interestingly, pipeline reversal and intevention interaction coefficients of the ARIMAX model are not statistically significant, suggesting the pipeline reversal has had no effect on WTI prices. It's possible that negative price pressures associated with macro events occuring in the wake of the pipeline reversal, such as the growth slowdown in China and  increase in US shale oil production, have outweighed the theorized positive effects of the pipeline reversal.   


```{r}

xreg <- df1[c("reversal", "brent", "decline", "post.int")]
arima1 <- auto.arima(df1$wti, xreg = xreg, allowmean = TRUE)
summary(arima1)
acf(arima1$residuals)


plot(arima1$residuals); plot(lm6$residuals, type = "l")


```

Overlaying the fitted values of the preffered OLS specification demonstrate that we could potentially generate a reasonable forecast from the data. However, doing so will require assumptions of the exogenous regressors, most importantly brent crude prices. The below code forms a prediction under the naive assumption that brent crude prices remain constant moving forward through time. The point forecast for WTI prices of \$53.60 by September of 2018 shows a significant gap between the realized value of \$70.23. To build a more reliable forecast, one would need to build a more accurate prediction of brent prices (although this seemingly defeats the purpose as most assumptions and regressors used to build a brent forecast can simply be used for a WTI forecast, because of arbitrage).  

```{r}

ggplot(df1, aes(x = DATE, y = wti))+ geom_line() + theme_bw()+
labs(title = "OLS Model WTI~ Brent + intervention terms", subtitle ="Fitted vs. Realized Values") + geom_line(aes(y = lm6$fitted.values), color = "red")

#Let's assume brent remains constant and forecast 9 steps
brent <- rep(df1$brent[156],9)
post.int <- seq(157:165)
reversal <- rep(1,9)
decline <- rep(1,9)
df1 <- data.frame(brent,post.int,reversal,decline)
forecast.lm6 <- predict(lm6, df1, n.ahead = 9)



```




